{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Eins Documentation", "text": ""}, {"location": "#introduction-one-tensor-operation-is-all-you-need", "title": "Introduction: One Tensor Operation is All You Need", "text": "<p>What if most of your machine learning model code could be replaced by a single operation? Eins gives you a powerful language to describe array manipulation, making it a one-stop shop for all of your AI needs.</p> <p>Let's say you want to compute batched pairwise distance between two batches of arrays of points:</p> <pre><code>from eins import EinsOp\nEinsOp('batch n1 d, batch n2 d -&gt; batch n1 n2',\n       combine='add', reduce='l2_norm')(pts1, -pts2)\n</code></pre> <p>If you're more interested in deep learning, here is the patch embedding from a vision transformer. That means breaking up an image into patches and then linearly embedding each patch.</p> <pre><code>from eins import EinsOp\npatchify = EinsOp('''b (n_p patch) (n_p patch) c, (patch patch c) emb -&gt; b (n_p n_p) emb''')\npatches = patchify(images, kernel)\n</code></pre>"}, {"location": "#installation", "title": "Installation", "text": "<p>Eins just has a few pure Python dependencies, and installation should be as easy as:</p> <pre><code>pip install eins\n</code></pre> <p>Eins works with anything that implements the Array API, and Eins explicitly promises to support NumPy, PyTorch, and JAX\u2014including full differentiability. You will need one of those libraries to actually use Eins operations.</p>"}, {"location": "#next-steps", "title": "Next Steps", "text": "<p>To get started, check out the tutorial, which walks you through the most important parts of Eins and shows how you can use Eins to manipulate data in a readable, reliable way.</p> <p>Advanced Eins explains advanced features and functionality\u2014not always essential, but if you want Eins to be a one-stop shop for all of your tensor manipulation needs you should give it a read.</p> <p>Finally, if you want to see exactly what's happening under the hood, you can consult the API documentation. Ideally, there would be no reason to read the API docs unless you're interested in the internals, but it's there if you need it.</p> <p>If you run into problems, think something should be done differently, or want to suggest improvements to Eins, feel free to file an issue.</p>"}, {"location": "in-depth/", "title": "Advanced Eins", "text": "<p>If you're totally new to Eins or <code>einops</code> and want to see what the fuss is about, read the tutorial. If you're interested in maximizing the power of Eins, you're in the right place!</p> <pre><code>from eins import EinsOp\n</code></pre>"}, {"location": "in-depth/#beyond-tensor-shapes", "title": "Beyond Tensor Shapes", "text": "<p>Consider an operation <code>a b, b c -&gt; a c</code>. This could be matrix multiplication, but it could also be pairwise distances or many other things. To control the computation that's being performed beyond the shapes of the inputs and output, Eins defines four kinds of functions that specify what's actually happening:</p>"}, {"location": "in-depth/#combinations", "title": "Combinations", "text": "<p>Combinations are, mathematically, functions that take two scalars and output a scalar. In Eins, combinations should be vectorized, taking in two arrays of the same shape and returning an array of that shape. The most common examples are <code>np.add</code> and <code>np.multiply</code>.</p> <p>Common examples: <code>'add'</code>, <code>'multiply'</code>, <code>'minimum'</code>, <code>'maximum'</code></p> <p>Danger</p> <p>Eins assumes that a combination is commutative and associative, and it makes no guarantees about the order your arrays are combined. If you supply custom functions, that responsibility is yours.</p>"}, {"location": "in-depth/#reductions", "title": "Reductions", "text": "<p>Reductions are essentially functions that take in a vector of any size and return a scalar, like <code>np.sum</code>. (These are sometimes called aggregations.) In Eins, they're functions that take an array and an axis and return an array with that axis removed.</p> <p>If you pass in a combination, Eins will essentially apply <code>functools.reduce</code> and use that combination to reduce the axis. In general, however, there are more efficient ways of doing the same thing: a folded <code>'add'</code> is just a slower <code>'sum'</code>, and a folded <code>hypot</code> is just a slower <code>l2-norm</code>.</p> <p>Common examples: <code>'sum'</code>, <code>'prod'</code>, <code>'l2_norm'</code>, <code>'min'</code>, <code>'max'</code>.</p> <p>Note the naming conventions, matching NumPy nomenclature. <code>np.max(arr, axis=0)</code> computes the max along axis 0, eliminating it. <code>np.maximum(arr1, arr2)</code> is the elementwise maximum between two arrays.</p> <p>Danger</p> <p>If you reduce more than once in a program, Eins assumes you know what you're doing and that the operation would be the same either way, like summing over two axes. If you supply a custom function, make sure there is only one potential output.</p>"}, {"location": "in-depth/#elementwise-operations", "title": "Elementwise Operations", "text": "<p>An elementwise operation should be thought of as a function that takes a scalar and outputs a scalar. Eins requires that the operation is vectorized, so it takes in an array and outputs an array of the same shape.</p> <p>Common examples: <code>'log'</code>, <code>'exp'</code>, <code>'tanh'</code>, <code>'square'</code>, <code>'sqrt'</code></p>"}, {"location": "in-depth/#transformations", "title": "Transformations", "text": "<p>Named after the <code>.transform</code> method in Pandas, transformations should be thought of mathematically as functions that take in a vector of any size and produce a vector of the same size. Think of sorting or standardization: you need multiple inputs for standardization to make sense, but at the end you haven't changed the shape of the array.</p> <p>In Eins, transformations take in a single array and <code>axis</code>, like reductions, but they don't eliminate the axis. For example, <code>np.sort(arr, axis=0)</code> is different than <code>np.sort(arr, axis=1)</code>, but both return an array of the same shape as <code>arr</code>.</p> <p>Just like a folded combination becomes a reduction, a scanned or accumulated combination becomes a transformation. Note that the way NumPy and other libraries notate these differs from the idea of a scan. <code>cumprod</code>, in Eins, is really just an alias for <code>cummultiply</code>, because Eins uses the combination rather than the reduction. If you have an array with elements <code>[a, b, c, d]</code> and an operator like <code>*</code>, then Eins computes</p> <pre><code>[a, a * b, (a * b) * c, ((a * b) * c) * d]\n</code></pre> <p>Common examples: <code>'sort'</code>, <code>'l2_normalize'</code>, <code>'min_max_normalize'</code></p>"}, {"location": "in-depth/#composing-functions", "title": "Composing Functions", "text": "<p>Eins uses <code>combine</code> and <code>reduce</code> arguments that specify how to combine inputs and how to reduce axes. The point of elementwise operations and transformations is that they can be composed with combinations and reductions.</p> <p>Functions are applied right-to-left, matching existing nomenclature and function composition. For example, if <code>'logaddexp'</code> weren't already a supported combination, you could replicate the functionality as <code>('log', 'add', 'exp')</code>. This computes the logarithm of the sum of the exponentials of the inputs.</p> <p>Similarly, if you wanted to compute root-mean-square error along an axis, you could use <code>reduce=('sqrt', 'mean', 'square')</code>. This is common enough to get its own name: <code>'l2_norm'</code>.</p>"}, {"location": "in-depth/#explicit-function-objects", "title": "Explicit Function Objects", "text": "<p>Eins supports a relatively sophisticated \"stringly-typed\" input format, as you've seen above. This means you rarely need any imports beyond <code>EinsOp</code>, and you can easily serialize the description of the operation, but it does also make it harder to know what functions Eins defines or use your own.</p> <p>If you prefer, you can instead pass in explicit objects: <code>Combination</code>, <code>Reduction</code>, <code>ElementwiseOp</code>, and <code>Transformation</code>. These are each base classes that you can implement yourself, but it's easiest to use the associated object exported from the base namespace: <code>Combinations</code>, <code>Reductions</code>, etc. These namespaces provide an autocomplete-friendly way of using these operations.</p> <p>Explicit objects are the only way to specify compositions with function syntax. If you pass in a callable to <code>combine</code> or <code>reduce</code>, Eins will assume it has the correct signature, but if you pass in <code>(my_func1, my_func2)</code> Eins has no way of knowing what's what. Instead, you can do:</p> Batched Kurtosis<pre><code>from eins import EinsOp, Reductions as R, ElementwiseOps as E\nfrom scipy.stats import kurtosis\n# kurtosis has signature (x, axis=0, ...)\n\nEinsOp('batch sample_size -&gt; batch', reduce=(E.abs, R.from_func(kurtosis)))\n</code></pre>"}, {"location": "in-depth/#standalone-operator-usage", "title": "Standalone Operator Usage", "text": "<p>For backend-agnostic code or simply as a wrapper for functionality Eins implements that isn't available in all libraries, there's no reason you can't just use the above functions outside of an EinsOp context:</p> <pre><code>from eins import Reductions as R, Transformations as T\n\n# 1.5-norm: somewhere between Manhattan and Euclidean distance\n# akin to torch.nn.functional.normalize, but no direct numpy equivalent\n\ndata = np.random.randn(128, 64)\n\nR.PowerNorm(1.5)(data, axis=1)\n\n# Normalize so the 1.5-norm is 1: same shape as input\nT.PowerNormalize(1.5)(data, axis=1)\n</code></pre>"}, {"location": "tutorial/", "title": "Tutorial", "text": "<p>If you're new to Eins, you're in the right place!</p> <p>Virtually everything Eins can do happens through a single import:</p> <pre><code>from eins import EinsOp\n</code></pre> <p>Let's go through some common deep learning operations as implemented in Eins, learning about what differentiates Eins from other einsum libraries along the way.</p>"}, {"location": "tutorial/#introduction", "title": "Introduction", "text": "<p><code>EinsOp</code> represents a function on tensors without any concrete inputs. (It's like a module in PyTorch or JAX.)</p> <pre><code>matmul = EinsOp('a b, b c -&gt; a c')\n</code></pre> <p>When you construct this operation, Eins essentially compiles a high-level program. This has two benefits over having a single function that takes in the expression and the inputs:</p> <ul> <li>The <code>EinsOp</code> is an object you can inspect, serialize, or manipulate however you like. This opens   the door for higher-level summaries of a program.</li> <li>It's common to apply a single operation to many inputs of the same shape. Any work Eins does in   parsing and interpreting your expression won't have to be repeated if you define the operation   once.</li> </ul>"}, {"location": "tutorial/#generalized-matrix-multiplication", "title": "Generalized Matrix Multiplication", "text": "<p>When you want to apply your operation, all you need to do is call it:</p> Matrix multiplication<pre><code># returns x @ y\nmatmul = EinsOp('a b, b c -&gt; a c')\nmatmul(x, y)\n</code></pre> <p>Eins should support any implementation of the Array API, but it specifically promises support for NumPy, PyTorch, and JAX. One of the best things about using Eins is that it frees you from having to remember the subtle differences in nomenclature and behavior between libraries.</p>"}, {"location": "tutorial/#batched-1x1-convolution", "title": "Batched 1x1 Convolution", "text": "<p>Eins shines when things get a little more interesting. Consider a batched 1x1 convolution that linearly maps channels in a BHWC image array to new channels.</p> 1\u00d71 Convolution<pre><code>EinsOp('''\nbatch height width chan_in, chan_in chan_out -&gt;\nbatch height width chan_out\n''')(images, kernel)\n</code></pre> <p>Just like normal Python code, using descriptive identifiers helps the reader understand what's going on. Eins allows arbitrary whitespace between commas and <code>-&gt;</code>.</p>"}, {"location": "tutorial/#reshaping", "title": "Reshaping", "text": "<p>Let's say we have images in the BHWC format like above, and we want to stack the rows into a single axis. To notate a flattened axis, we use <code>(height width)</code>:</p> Flatten inner axes<pre><code># equivalent to ims.reshape(batch, height * width, channels)\nEinsOp('''batch height width channels -&gt;\n          batch (height width) channels''')(ims)\n</code></pre> <p>You can think of the parentheses as essentially flattening.</p>"}, {"location": "tutorial/#fancy-reshaping", "title": "Fancy Reshaping", "text": "<p>Here's something you won't find in other libraries: let's say we have a batch of square images that we've flattened using something like the above, and we want to reverse it. Just tell Eins that the two axes are the same size, and it'll figure out what to do. You can either do that explicitly, by using <code>=</code>, or implicitly, by repeating an axis name within a single tensor.</p> <p>Unflatten batch of square images<pre><code>EinsOp('b (h w=h) c -&gt; b h w c')(ims)\nEinsOp('b (h h) c -&gt; b h h c')(ims)\n</code></pre> Eins defaults to matching up duplicate axes in the same order they appear. If you want to transpose after the unflatten, you need to use the explicit syntax.</p> <p>Eins also understands explicit constants:</p> Unflattening 3D points<pre><code>EinsOp('b (n 3) -&gt; b n 3')(pts)\n</code></pre>"}, {"location": "tutorial/#strided-convolutionpatch-encoder", "title": "Strided Convolution/Patch Encoder", "text": "<p>The patch encoder in a vision transformer is a specific kind of strided convolution, breaking the image into p \u00d7 p squares and then linearly embedding each of them. Despite the complexity, Eins can figure everything out without any explicit shape information:</p> Patch encoder (ViT)<pre><code>EinsOp('''batch (num_patch p) (num_patch p) chan_in,\n(p p chan_in) chan_out -&gt;\nbatch (num_patch num_patch) chan_out''')(images, kernel)\n</code></pre> <p>Eins knows <code>chan_in</code> from <code>images</code>, can use that plus the knowledge that the patches are square to deduce <code>p</code>, and then can figure out <code>num_patch</code> from there.</p> <p>The constraint system Eins supports saves your fingers from having to type out redundant information and saves your brain time debugging subtle logic errors.</p>"}, {"location": "tutorial/#specifying-shapes", "title": "Specifying Shapes", "text": "<p>Despite these efforts to figure out what you want, sometimes there are multiple potential values for a symbol, and Eins needs your help to figure out which one to do. Literal integer constants are succinct, but they lose some of the flexibility and readability of named axes. You can get the best of both worlds by passing in explicit axis mappings, using either the <code>symbol_values</code> argument or the <code>=</code> sign:</p> Unflatten non-square axes<pre><code>EinsOp('b (h w) c -&gt; b h w c', symbol_values={'h': 4, 'w': 16})(ims)\nEinsOp('b (h=4 w=16) c -&gt; b h w c')(ims)\n</code></pre> <p>You only need one of <code>h</code> and <code>w</code> specified, because Eins can deduce the other one, but sometimes it's nice to have an extra check that the input you give Eins is the shape you think it is.</p>"}, {"location": "tutorial/#advanced-shape-manipulation-with-sum-axes", "title": "Advanced Shape Manipulation with Sum Axes", "text": "<p>An axis of shape <code>(h w)</code> has h \u00d7 w values, and in fact Eins will let you write that as <code>h*w</code> if you prefer. Eins also supports <code>h+w</code>, which generalizes the notion of concatenation.</p>"}, {"location": "tutorial/#concatenation", "title": "Concatenation", "text": "Concatenate inputs along second axis<pre><code>EinsOp('b n1, b n2 -&gt; b n1+n2')(x, y)\n</code></pre>"}, {"location": "tutorial/#splitting", "title": "Splitting", "text": "<p>You can also have <code>+</code> signs in the input, which lets you slice and dice arrays in a readable way:</p> Split along axis<pre><code>EinsOp('5+a b -&gt; a b')(x)\n</code></pre> Advanced Example: PCA <p>PCA, more specifically Singular value decomposition is one real-world example of when you might need this. Let's say you have three arrays U, S, V of shapes M \u00d7 R, R, and R \u00d7 N, which is what <code>np.linalg.svd</code> would return. You want to approximate the combined product, of shape M \u00d7 N, by taking only the first 5 values along the R axis:</p> Truncated SVD<pre><code>u, s, v = np.linalg.svd(np.random.randn(8, 7))\ntruncated_rank = 2\nop = EinsOp('m t+_1, t+_2, t+_3 n -&gt; m n', symbol_values={'t': truncated_rank})\nop(u, s, v)\n</code></pre> <p>Because <code>t</code> is shared across the different inputs, Eins uses that part of the split to make the output. When in doubt about how to interpret something, there's no harm in breaking it up, but this showcases the deduction ability Eins has.</p>"}, {"location": "tutorial/#beyond-einsum", "title": "Beyond Einsum", "text": "<p>Deep learning would not be a very exciting field if the only things you did were matrix multiplication and rearranging elements in memory. Luckily, Eins supports a lot more, and moreover it does so without long docs pages of different single-purpose functions to learn.</p> <p>Note</p> <p>This tutorial won't cover everything about this part of Eins: consult Advanced Eins if you want a more in-depth tour of this area of the library's functionality.</p> <p>To understand how Eins represents computation, let's think about how matrix multiplication works. A matrix multiplication of two matrices M and N with shapes A \u00d7 B and B \u00d7 C can be broken down as such:</p> \\[ (MN)_{ac} = \\sum_{b=1}^{B} (M_{ab} \\cdot N_{bc}) \\] <p>We first combine M and N together by a broadcasted elementwise multiplication, lining up the two matrices along the B axis. Then, we eliminate the B axis by summing over all of its values. If you prefer Python, here's code expressing this idea:</p> Matrix multiplication as broadcasted product and sum over axis<pre><code># reshape AB to a x b x 1 and reshape BC to 1 x b x c\n# then they broadcast to a x b x c\nABC = np.multiply(AB[:, :, None], BC[None, :, :])\n# sum over B axis\nAC = ABC.sum(axis=1)\n# AC is equivalent to AB @ BC\n</code></pre> <p>We can generalize matrix multiplication by replacing <code>.multiply</code> and <code>.sum</code> with other functions that have the same signature. Using NumPy as an example, if you go through their API you'll find a couple functions that can be subsituted for the ones above:</p> Default Alternatives Signature <code>np.multiply</code> <code>np.add</code>, <code>np.minimum</code>, <code>np.maximum</code> <code>f(Array, Array) -&gt; Array</code>, with all array shapes equal <code>np.sum</code> <code>np.prod</code>, <code>np.min</code>, <code>np.max</code> <code>f(Array, axis: int) -&gt; Array</code>, removing the axis <p>Eins calls the first kind of function a combination and the second kind a reduction. The defaults are <code>combine='multiply'</code> and <code>reduce='sum'</code>, which is why we haven't needed them for matrix multiplication.</p>"}, {"location": "tutorial/#adding-a-bias-term", "title": "Adding a Bias Term", "text": "<p>Let's say we just applied a linear layer to get outputs of shape <code>batch dim</code>. We can apply a broadcasted sum with a bias parameter of shape <code>dim</code> by supplying the <code>combine</code> argument:</p> Bias Term<pre><code>EinsOp('batch dim, dim -&gt; batch dim', combine='add')(linear, bias)\n</code></pre> <p>This would happen automatically by broadcasting, but when broadcasting doesn't work it's often quite error-prone to manually line up shapes.</p>"}, {"location": "tutorial/#pairwise-distance", "title": "Pairwise Distance", "text": "<p>Let's say we have batches of <code>d</code>-dimensional points and we want to compute the pairwise distance between points in the same batch:</p> Batched Pairwise Distance<pre><code>EinsOp('batch n1 d, batch n2 d -&gt; batch n1 n2',\n       combine='add', reduce='l2_norm')(pts1, -pts2)\n</code></pre> <p>We're still using addition. Eins does not promise any particular ordering of inputs, so using combinations that aren't commutative and associative can lead to surprising problems. Negating one of the inputs is a more reliable way of computing the difference. (For some reason, it's also often faster!)</p> <p>That would give us the array of vectors between points, of shape <code>batch n1 n2 d</code>. We want a shape of <code>batch n1 n2</code>, so we have to eliminate the <code>d</code> axis.</p> <p>We do this by computing the Euclidean norm: the square root of the sum of the squares of the values along the axis. This is called the \\(L_2\\) norm, hence the name.</p>"}, {"location": "tutorial/#batched-mean-huber-loss", "title": "Batched Mean Huber Loss", "text": "<p>The literals that Eins accepts are documented properly in the type system, so you should get a handy autocomplete for a name like <code>l2_norm</code>. The time will come when one of those options isn't appropriate, however. Eins supports various ways of supplying custom functions.</p> <p>One solution is to simply pass in your own function. Combinations should have two positional arguments and output an array of the same size, and custom reductions should take in a single positional argument and either <code>axis</code> or <code>dim</code> as keyword arguments.</p> Batched Mean Huber Loss<pre><code>from torch.nn.functional import huber_loss\n\nEinsOp('batch out_features, batch out_features -&gt; batch',\n       combine=huber_loss, reduce='mean')(y, y_hat)\n</code></pre>"}, {"location": "tutorial/#composable-nonlinear-math", "title": "Composable Nonlinear Math", "text": "<p>There's often a better way than passing in a custom function. We've only discussed operations that work on the level of shapes: either taking two arrays and combining them, or taking a single array and reducing an axis. There are two other kinds of operations Eins defines that don't modify the shape of the output:</p> <ul> <li>Elementwise functions are basically just functions from real numbers to real numbers that you   can batch arbitrarily. Examples are <code>np.sin</code>, <code>np.abs</code>, and <code>np.exp</code>. They have the signature   <code>f(Array) -&gt; Array</code>, and mathematically they're functions from a scalar to a scalar.</li> <li>Transformations use an axis, but don't eliminate it. Examples are <code>np.sort</code>, <code>np.flip</code>,   <code>np.roll</code>, and normalization. They have the signature <code>f(Array, axis: int) -&gt; Array</code>, and   mathematically they're functions from a vector to a vector.</li> </ul> <p>Eins implements a library of these functions to go along with combinations and reductions. Combining them lets you make new functions that are easy to reason about and framework-agnostic. Passing in a sequence of operations applies them right-to-left, like functions. For example, although Eins already has <code>'logaddexp'</code>, you could instead pass in <code>combine=('log', 'add', 'exp')</code>.</p> <p>For combinations, you can pass in any number of elementwise operations, sandwiching a single combination operation. For reductions, we do know an axis, so transformations are allowed on top of elementwise operations and reductions. Once you reduce the axis, it's gone, so a transformation or reduction can't come after the first reduction.</p>"}, {"location": "tutorial/#approximate-numerical-equality", "title": "Approximate Numerical Equality", "text": "<p>It's hard to give an example for \"a common function Eins doesn't support\", because the hope is that Eins supports what you're likely to use! So, even if a little contrived, let's say you're interested in finding whether two arrays are all equal within an epsilon. We could (and probably should) supply a custom function, but we can instead implement it using elementwise functions: rounding answers so only values above 1 show up.</p> Batched Approximate Numerical Equality<pre><code>eps = 1e-6\neps_apart = EinsOp('batch dim, batch dim -&gt; batch',\n                    combine=('round', 'abs', 'add'),\n                    reduce='max')\neps_apart(x / eps, y / eps) == 0\n</code></pre>"}, {"location": "tutorial/#softmax-with-labeled-axes", "title": "Softmax with Labeled Axes", "text": "<p>Elementwise functions aren't something you generally want to use outside of composition with other functions in Eins. If you know that you're using a JAX array, <code>jnp.sin</code> is going to be just as clean and readable as anything Eins would provide. Transformations distinguish an axis, however, so the machinery Eins has for working with shapes makes those transformations easier to understand. Eins lets you write out transformations in a clearer way than frameworks that use indexed axes.</p> <p>Consider the transformer attention operation. The Annotated Transformer gives this equation and this code for the softmax in attention:</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}} \\right) V \\] <pre><code>def attention(query, key, value):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    p_attn = scores.softmax(dim=-1)\n    return torch.matmul(p_attn, value), p_attn\n</code></pre> <p>It's legitimately not clear what axis <code>scores.softmax(dim=-1)</code> means in this context, and it's not indicated in the equation either. The Eins version clearly indicates what dimension we're applying softmax to:</p> Softmax with Labeled Axes<pre><code>p_attn = EinsOp('batch q_seq k_seq heads', transform={'k_seq': 'softmax'})(scores)\n</code></pre> <p>In many applications of attention, <code>q_seq</code> and <code>k_seq</code> are the same size: there would be no indication that you were performing softmax incorrectly until things stopped working well.</p> <p>When you give <code>EinsOp</code> an expression without an arrow, it considers your input as a single array, and leaves the shape unchanged. You can pass in a <code>transform</code> mapping to indicate how you want to transform an axis. Eins does not guarantee an order of transformations, so if you do use multiple transformations make sure they're commutative.</p>"}, {"location": "tutorial/#typed-functions", "title": "Typed Functions", "text": "<p>Passing in strings is quick and doesn't require polluting the namespace, but it's not always easy to know what Eins allows you to write. For the price of a few more imports, you can use something closer to strong typing than string typing:</p> <pre><code>from eins import ElementwiseOps as E\nfrom eins import Reductions as R\nfrom eins import Transformations as T\nfrom eins import Combinations as C\n</code></pre> <p>Feel free to use other names, but the analogy to <code>import torch.nn.functional as F</code> is a good one: these are all simple namespaces with autocomplete-friendly interfaces for accessing functions. The different functions are all dataclasses that can be serialized and inspected easily.</p> <p>These namespaces support some customizations that can't be done through strings alone. For example, we can use softmax with temperature as <code>T.Softmax(temperature=2)</code>. The <code>l2_norm</code> we saw above can actually be any power, so we could do Manhattan distance through <code>l1_norm</code> or even <code>2.4_norm</code>. At that point, however, you're probably better off writing <code>R.PowerNorm(power=2.4)</code>.</p> <p>In addition, because these namespaces are typed, Eins knows what you want to do with them, and so these let you provide custom functions in more situations. Eins will only accept raw functions if they're the only input, because otherwise it's unclear what the signature is.</p>"}, {"location": "tutorial/#next-steps", "title": "Next Steps", "text": "<p>That's everything you need to get good use out of Eins! Feel free to check out Advanced Eins if you want to learn more.</p>"}, {"location": "stylesheets/readme/", "title": "Readme", "text": ""}, {"location": "stylesheets/readme/#geist-sans-geist-mono", "title": "Geist Sans &amp; Geist Mono", "text": "<p>Geist is a new font family for Vercel, created by Vercel in collaboration with Basement Studio.</p> <p>Geist Sans is a sans-serif typeface designed for legibility and simplicity. It is a modern, geometric typeface that is based on the principles of classic Swiss typography. It is designed to be used in headlines, logos, posters, and other large display sizes.</p> <p>Geist Mono is a monospaced typeface that has been crafted to be the perfect partner to Geist Sans. It is designed to be used in code editors, diagrams, terminals, and other textbased interfaces where code is represented.</p>"}, {"location": "stylesheets/readme/#installation", "title": "Installation", "text": "<p>Download the latest release from the releases page and install the fonts on your system. * Download Geist Sans * Download Geist Mono</p>"}, {"location": "stylesheets/readme/#license", "title": "License", "text": "<p>The Geist font family is free and open sourced under the SIL Open Font License.</p>"}, {"location": "stylesheets/readme/#inspiration", "title": "Inspiration", "text": "<p>Geist has been influenced and inspired by the following typefaces: Inter, Univers, SF Mono, SF Pro, Suisse International, ABC Diatype Mono, and ABC Diatype. We thank the creators of these typefaces for their craft.</p>"}]}