"""
Implementation of Multi-Headed Attention (MHA) in Flax using Eins.

https://nn.labml.ai/transformers/mha.html gives a PyTorch equivalent.
"""

import flax.linen as nn
# TODO how to do softmax?
